{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting Threads Post Data"
      ],
      "metadata": {
        "id": "4xrb-Vg7xH29"
      },
      "id": "4xrb-Vg7xH29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bdc4b7",
      "metadata": {
        "scrolled": true,
        "id": "d9bdc4b7",
        "outputId": "76d9037b-43dc-471d-f11a-21298dc98572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "     ---------------------------------------- 9.7/9.7 MB 7.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "     -------------------------------------- 481.7/481.7 kB 2.3 MB/s eta 0:00:00\n",
            "Collecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Collecting typing_extensions~=4.9\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
            "Collecting websocket-client~=1.8\n",
            "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 58.8/58.8 kB 1.5 MB/s eta 0:00:00\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting exceptiongroup\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
            "Collecting attrs>=23.2.0\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "     ---------------------------------------- 63.0/63.0 kB 1.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
            "Collecting sniffio>=1.3.0\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 58.3/58.3 kB 1.5 MB/s eta 0:00:00\n",
            "Installing collected packages: websocket-client, typing_extensions, sniffio, h11, exceptiongroup, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed attrs-24.2.0 exceptiongroup-1.2.2 h11-0.14.0 outcome-1.3.0.post0 selenium-4.27.1 sniffio-1.3.1 trio-0.27.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script wsdump.exe is installed in 'C:\\Users\\abhip\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d675704a",
      "metadata": {
        "id": "d675704a"
      },
      "source": [
        "### Extracting Basic Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361a8a9f",
      "metadata": {
        "scrolled": true,
        "id": "361a8a9f",
        "outputId": "710ed6e0-a676-4fc0-95b8-05f36883650d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Username: syeedthedoyenne\n",
            "Post Text: Texas just gifted Trump 355,000 acres for deportation camps.I want you to think about that critically. \n",
            "I'm taking a nap while you do that.\n",
            "Likes: 1.4K\n",
            "Replies: 80\n",
            "Reposts: 229\n",
            "Shares: 57\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "#     print(counts_div)\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "#         print(counts)\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 1 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 1 else None\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post Text:\", post_text)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b1ae18",
      "metadata": {
        "id": "f0b1ae18"
      },
      "source": [
        "### Adding Post Date Time Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec33cb8c",
      "metadata": {
        "scrolled": true,
        "id": "ec33cb8c",
        "outputId": "06811266-f06c-4d56-ae77-f0f3dce07b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Username: josephgoldner62\n",
            "Post Text: Arizona officials caught changing ballots , have been arrested!\n",
            "Post Date: 2024-11-09T19:38:36.000Z\n",
            "Likes: 2K\n",
            "Replies: 94\n",
            "Reposts: 103\n",
            "Shares: 51\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@josephgoldner62/post/DCKaLGwvvqO\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract date of post\n",
        "    date_div = soup.find('time')  # 'time' tags are often used for dates/times\n",
        "    post_date = date_div['datetime'] if date_div and 'datetime' in date_div.attrs else date_div.text.strip() if date_div else None\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 2 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 3 else None\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post Text:\", post_text)\n",
        "    print(\"Post Date:\", post_date)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b5f044",
      "metadata": {
        "scrolled": true,
        "id": "e3b5f044",
        "outputId": "35fcb706-65e8-451d-85cf-fb2a9a87a033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Username: syeedthedoyenne\n",
            "Post Text: Texas just gifted Trump 355,000 acres for deportation camps.I want you to think about that critically. \n",
            "I'm taking a nap while you do that.\n",
            "Post Date: 2024-11-20\n",
            "Post Time: 22:31:34.000\n",
            "Likes: 1.4K\n",
            "Replies: 80\n",
            "Reposts: 229\n",
            "Shares: 57\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract date of post\n",
        "    date_div = soup.find('time')  # 'time' tags are often used for dates/times\n",
        "    post_date = date_div['datetime'] if date_div and 'datetime' in date_div.attrs else None\n",
        "\n",
        "    # Split the date and time\n",
        "    post_date_only = None\n",
        "    post_time_only = None\n",
        "    if post_date:\n",
        "        post_date_only, post_time_only = post_date.split('T')\n",
        "        post_time_only = post_time_only.rstrip('Z')  # Remove 'Z' for better readability\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 2 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 3 else None\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post Text:\", post_text)\n",
        "    print(\"Post Date:\", post_date_only)\n",
        "    print(\"Post Time:\", post_time_only)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4763e56a",
      "metadata": {
        "id": "4763e56a"
      },
      "source": [
        "### Adding Post URL Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232e56b1",
      "metadata": {
        "scrolled": true,
        "id": "232e56b1",
        "outputId": "1bd88ec5-41e6-4ad8-dff9-c6b3e965d57b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post_URL: https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\n",
            "Username: syeedthedoyenne\n",
            "Post_Text: Texas just gifted Trump 355,000 acres for deportation camps.I want you to think about that critically. \n",
            "I'm taking a nap while you do that.\n",
            "Post_Date: 2024-11-20\n",
            "Post_Time (HH:MM): 22:31\n",
            "Likes: 1.4K\n",
            "Replies: 80\n",
            "Reposts: 229\n",
            "Shares: 57\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract date of post\n",
        "    date_div = soup.find('time')  # 'time' tags are often used for dates/times\n",
        "    post_date = date_div['datetime'] if date_div and 'datetime' in date_div.attrs else None\n",
        "\n",
        "    # Split the date and extract hours and minutes\n",
        "    post_date_only = None\n",
        "    post_time_only = None\n",
        "    if post_date:\n",
        "        post_date_only, post_time_only = post_date.split('T')\n",
        "        post_time_only = post_time_only.rstrip('Z')[:5]  # Keep only HH:MM\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 2 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 3 else None\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"Post_URL:\", url)\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post_Text:\", post_text)\n",
        "    print(\"Post_Date:\", post_date_only)\n",
        "    print(\"Post_Time (HH:MM):\", post_time_only)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366cebfb",
      "metadata": {
        "id": "366cebfb"
      },
      "source": [
        "### Accessed Date and Time Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1e166c",
      "metadata": {
        "scrolled": true,
        "id": "cd1e166c",
        "outputId": "b38a8660-b36c-43ed-8f06-033f11d34eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL: https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\n",
            "Username: syeedthedoyenne\n",
            "Post Text: Texas just gifted Trump 355,000 acres for deportation camps.I want you to think about that critically. \n",
            "I'm taking a nap while you do that.\n",
            "Post Date: 2024-11-20\n",
            "Post Time (HH:MM): 22:31\n",
            "Likes: 1.4K\n",
            "Replies: 80\n",
            "Reposts: 229\n",
            "Shares: 57\n",
            "Accessed Date: 2024-12-01\n",
            "Accessed Time: 03:49\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract date of post\n",
        "    date_div = soup.find('time')  # 'time' tags are often used for dates/times\n",
        "    post_date = date_div['datetime'] if date_div and 'datetime' in date_div.attrs else None\n",
        "\n",
        "    # Split the date and extract hours and minutes\n",
        "    post_date_only = None\n",
        "    post_time_only = None\n",
        "    if post_date:\n",
        "        post_date_only, post_time_only = post_date.split('T')\n",
        "        post_time_only = post_time_only.rstrip('Z')[:5]  # Keep only HH:MM\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 2 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 3 else None\n",
        "\n",
        "    # Get current date and time\n",
        "    accessed_date = datetime.now().strftime('%Y-%m-%d')\n",
        "    accessed_time = datetime.now().strftime('%H:%M')\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"URL:\", url)\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post Text:\", post_text)\n",
        "    print(\"Post Date:\", post_date_only)\n",
        "    print(\"Post Time:\", post_time_only)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "    print(\"Accessed Date:\", accessed_date)\n",
        "    print(\"Accessed Time:\", accessed_time)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fdd5ef5",
      "metadata": {
        "id": "0fdd5ef5"
      },
      "source": [
        "### Adding Verified User Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8a9fbd",
      "metadata": {
        "scrolled": true,
        "id": "3c8a9fbd",
        "outputId": "07b25dd3-6152-48b6-8f69-c31d5940a21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL: https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\n",
            "Username: syeedthedoyenne\n",
            "Post Text: Texas just gifted Trump 355,000 acres for deportation camps.I want you to think about that critically. \n",
            "I'm taking a nap while you do that.\n",
            "Post Date: 2024-11-20\n",
            "Post Time (HH:MM): 22:31\n",
            "Likes: 1.4K\n",
            "Replies: 80\n",
            "Reposts: 229\n",
            "Shares: 57\n",
            "Verified User: FALSE\n",
            "Accessed Date: 2024-12-01\n",
            "Accessed Time: 04:05\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract date of post\n",
        "    date_div = soup.find('time')  # 'time' tags are often used for dates/times\n",
        "    post_date = date_div['datetime'] if date_div and 'datetime' in date_div.attrs else None\n",
        "\n",
        "    # Split the date and extract hours and minutes\n",
        "    post_date_only = None\n",
        "    post_time_only = None\n",
        "    if post_date:\n",
        "        post_date_only, post_time_only = post_date.split('T')\n",
        "        post_time_only = post_time_only.rstrip('Z')[:5]  # Keep only HH:MM\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 2 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 3 else None\n",
        "\n",
        "    # Check if the user is verified\n",
        "    verified_user = \"FALSE\"\n",
        "    verified_icon = soup.find('div', class_='x1i10hfl')  # Adjust class name based on verified icon\n",
        "    if verified_icon and 'verified' in verified_icon.attrs.get('class', []):\n",
        "        verified_user = \"TRUE\"\n",
        "\n",
        "    # Get current date and time\n",
        "    accessed_date = datetime.now().strftime('%Y-%m-%d')\n",
        "    accessed_time = datetime.now().strftime('%H:%M')\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"URL:\", url)\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post Text:\", post_text)\n",
        "    print(\"Post Date:\", post_date_only)\n",
        "    print(\"Post Time (HH:MM):\", post_time_only)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "    print(\"Verified User:\", verified_user)\n",
        "    print(\"Accessed Date:\", accessed_date)\n",
        "    print(\"Accessed Time:\", accessed_time)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abaa8df5",
      "metadata": {
        "scrolled": true,
        "id": "abaa8df5",
        "outputId": "446b699b-09ea-41ac-d8c6-4c6a1e3a24a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL: https://www.threads.net/@nardowick/post/CuWgvpHuAr3\n",
            "Username: nardowick\n",
            "Post Text: Threads\n",
            "Post Date: 2023-07-06\n",
            "Post Time (HH:MM): 09:59\n",
            "Likes: 3.4K\n",
            "Replies: 602\n",
            "Reposts: 321\n",
            "Shares: None\n",
            "Verified User: true\n",
            "Accessed Date: 2024-12-01\n",
            "Accessed Time: 04:12\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Step 1: Set up Selenium WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Optional: Run in headless mode\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "service = Service('C:\\\\webdriver\\\\chromedriver.exe')  # Replace with your chromedriver path\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Step 2: Open the Threads post URL\n",
        "url = \"https://www.threads.net/@nardowick/post/CuWgvpHuAr3\"\n",
        "driver.get(url)\n",
        "time.sleep(5)  # Wait for the page to load fully\n",
        "\n",
        "# Step 3: Parse the loaded page with BeautifulSoup\n",
        "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "# Step 4: Extract Data\n",
        "try:\n",
        "    # Extract username\n",
        "    username = soup.find('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xp07o12 xzmqwrg x1citr7e x1kdxza xt0b8zv').text.strip()\n",
        "\n",
        "    # Extract post text\n",
        "    post_text_div = soup.find('div', class_='x1a6qonq x6ikm8r x10wlt62 xj0a0fe x126k92a x6prxxf x7r5mf7')  # Adjust the class name as necessary\n",
        "    post_text = post_text_div.text.strip() if post_text_div else None\n",
        "\n",
        "    # Extract date of post\n",
        "    date_div = soup.find('time')  # 'time' tags are often used for dates/times\n",
        "    post_date = date_div['datetime'] if date_div and 'datetime' in date_div.attrs else None\n",
        "\n",
        "    # Split the date and extract hours and minutes\n",
        "    post_date_only = None\n",
        "    post_time_only = None\n",
        "    if post_date:\n",
        "        post_date_only, post_time_only = post_date.split('T')\n",
        "        post_time_only = post_time_only.rstrip('Z')[:5]  # Keep only HH:MM\n",
        "\n",
        "    # Extract like and reply counts\n",
        "    counts_div = soup.find('div', class_='x4vbgl9 xp7jhwk x1k70j0n')  # Adjust based on actual structure\n",
        "    likes = None\n",
        "    replies = None\n",
        "    reposts = None\n",
        "    shares = None\n",
        "    if counts_div:\n",
        "        counts = counts_div.find_all('span', class_='x17qophe x10l6tqk x13vifvy')\n",
        "        likes = counts[0].text.strip() if len(counts) > 0 else None\n",
        "        replies = counts[1].text.strip() if len(counts) > 1 else None\n",
        "        reposts = counts[2].text.strip() if len(counts) > 2 else None\n",
        "        shares = counts[3].text.strip() if len(counts) > 3 else None\n",
        "\n",
        "    # Check if the user is verified\n",
        "    verified_user = \"FALSE\"\n",
        "    verified_icon = soup.find('span', class_='xsgj6o6 xvijh9v')  # Adjust class name based on verified icon\n",
        "    if verified_icon:\n",
        "        verified_user = \"TRUE\"\n",
        "\n",
        "    # Get current date and time\n",
        "    accessed_date = datetime.now().strftime('%Y-%m-%d')\n",
        "    accessed_time = datetime.now().strftime('%H:%M')\n",
        "\n",
        "    # Print extracted data\n",
        "    print(\"URL:\", url)\n",
        "    print(\"Username:\", username)\n",
        "    print(\"Post Text:\", post_text)\n",
        "    print(\"Post Date:\", post_date_only)\n",
        "    print(\"Post Time (HH:MM):\", post_time_only)\n",
        "    print(\"Likes:\", likes)\n",
        "    print(\"Replies:\", replies)\n",
        "    print(\"Reposts:\", reposts)\n",
        "    print(\"Shares:\", shares)\n",
        "    print(\"Verified User:\", verified_user)\n",
        "    print(\"Accessed Date:\", accessed_date)\n",
        "    print(\"Accessed Time:\", accessed_time)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error extracting data:\", e)\n",
        "\n",
        "# Step 5: Close the WebDriver\n",
        "driver.quit()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the collected data to a DataFrame and save it to CSV\n",
        "output_df = pd.DataFrame(output_data)\n",
        "output_df.to_csv(\"scraped_threads_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "K1pAf6uvwUSX"
      },
      "id": "K1pAf6uvwUSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad91766c",
      "metadata": {
        "id": "ad91766c"
      },
      "source": [
        "### Extracting Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed583f90",
      "metadata": {
        "scrolled": true,
        "id": "ed583f90",
        "outputId": "e16762ca-a729-4e05-926d-3aeafcb18f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed data (first 10 rows):\n",
            "                                                 URL             Username  \\\n",
            "0  https://www.threads.net/@thompsonphil60/post/D...       thompsonphil60   \n",
            "1  https://www.threads.net/@allhallowswitchh/post...     allhallowswitchh   \n",
            "2  https://www.threads.net/@serenablue2024/post/D...  naina.chauhan.07.29   \n",
            "3  https://www.threads.net/@currentusupdates/post...     currentusupdates   \n",
            "4  https://www.threads.net/@nya360_/post/DCfF-PpyT6w              nya360_   \n",
            "5  https://www.threads.net/@stocks.tom/post/DCOwW...           stocks.tom   \n",
            "6  https://www.threads.net/@angelique.ambers/post...     angelique.ambers   \n",
            "7  https://www.threads.net/@aletheia_8/post/DCKPF...           aletheia_8   \n",
            "8  https://www.threads.net/@siriplaymastery/post/...  samegoddifferentday   \n",
            "9  https://www.threads.net/@nya360_/post/DCNIHV2y...              nya360_   \n",
            "\n",
            "                                           Post Text   Post Date  \\\n",
            "0  I guess they need to bring back that old song ...  2024-11-18   \n",
            "1                              LMAOOOOOOOOOOOOOOOOOO  2024-11-20   \n",
            "2                                         Gm🫰🖤💫🩷🫰❤️🦋  2024-11-30   \n",
            "3  Elon Musk Bans DreamWorks Animation From X For...  2024-11-18   \n",
            "4  BREAKING: PM  NETANYAHU HOSPITALISEDHe suffere...  2024-11-17   \n",
            "5  I don't know about Food Lion or Publix but Kro...  2024-11-11   \n",
            "6  Sephora donated BIG to Trump’s campaign… Ulta ...  2024-11-10   \n",
            "7  🚨BREAKING NOW🚨Ella Emhoff, Kamala’s step-daugh...  2024-11-09   \n",
            "8  Visa stopped posting on Twitter.\\nEtsy stopped...  2024-11-05   \n",
            "9  Breaking 🚨🚨 NATO Secretary General Mark Rutte ...  2024-11-10   \n",
            "\n",
            "  Post Time (HH:MM) Likes Replies Reposts Shares  Verified User Accessed Date  \\\n",
            "0             11:34   761      60      55     17           True    2024-12-01   \n",
            "1             01:32  1.7K     256      76     32           True    2024-12-01   \n",
            "2             06:07   151      11       2    NaN          False    2024-12-01   \n",
            "3             01:43   990      56      13      6          False    2024-12-01   \n",
            "4             20:26    82      63       9     18           True    2024-12-01   \n",
            "5             12:09   888      86      38     51          False    2024-12-01   \n",
            "6             13:13  1.3K     127     273     76           True    2024-12-01   \n",
            "7             18:01   482     251      33    216           True    2024-12-01   \n",
            "8             21:55  6.4K     439     298     59           True    2024-12-01   \n",
            "9             20:58   141     112       4      9           True    2024-12-01   \n",
            "\n",
            "  Accessed Time         Code  \n",
            "0         12:23  DCgt36GttDq  \n",
            "1         12:24  DCkyrbCvuvZ  \n",
            "2         12:24  DCegZjZxQPp  \n",
            "3         12:24  DCfqV61sbcA  \n",
            "4         12:24  DCfF-PpyT6w  \n",
            "5         12:24  DCOwW1XxrBr  \n",
            "6         12:24  DCMS30Pxzu8  \n",
            "7         12:24  DCKPF8LpB3e  \n",
            "8         12:24  DCAWnYUPvqk  \n",
            "9         12:25  DCNIHV2yNc0  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the input CSV file\n",
        "df = pd.read_csv(\"scraped_threads_data.csv\")\n",
        "\n",
        "# Function to extract the code from a Threads URL\n",
        "def extract_code(threads_url):\n",
        "    if pd.notnull(threads_url):  # Check if the URL is not null\n",
        "        match = re.search(r'/post/([a-zA-Z0-9_-]+)', threads_url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Apply the function to the Threads URL column and create a new Code column\n",
        "df['Code'] = df['URL'].apply(extract_code)\n",
        "\n",
        "# Preview the processed data\n",
        "print(\"Processed data (first 10 rows):\")\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f4d79f",
      "metadata": {
        "id": "32f4d79f"
      },
      "outputs": [],
      "source": [
        "output_df = pd.DataFrame(df)\n",
        "output_df.to_csv(\"scraped_threads_withCodes.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PVpQQT2NxTsP"
      },
      "id": "PVpQQT2NxTsP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting Fake Data"
      ],
      "metadata": {
        "id": "EEem_iLB4emR"
      },
      "id": "EEem_iLB4emR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Politifact Data Scraping**\n"
      ],
      "metadata": {
        "id": "Kh_gDO8_iS1q"
      },
      "id": "Kh_gDO8_iS1q"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Base URL pattern for the pages\n",
        "base_url_pattern = \"https://www.politifact.com/factchecks/list/?page={}&speaker=threads-posts\"\n",
        "\n",
        "def scrape_fact_checks(page_start, page_end):\n",
        "    fact_checks = []\n",
        "    for page in range(page_start, page_end + 1):\n",
        "        url = base_url_pattern.format(page)\n",
        "        print(f\"Scraping page {page}: {url}\")\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Locate all articles with claims\n",
        "        articles = soup.find_all('article', class_='m-statement')\n",
        "\n",
        "        if not articles:\n",
        "            print(f\"No articles found on page {page}.\")\n",
        "            continue\n",
        "\n",
        "        for article in articles:\n",
        "            # Extract claim text\n",
        "            claim_div = article.find('div', class_='m-statement__quote')\n",
        "            claim = claim_div.get_text(strip=True) if claim_div else \"Claim not found\"\n",
        "\n",
        "            # Extract the URL of the detailed fact-check page\n",
        "            link_tag = article.find('a', class_=False)\n",
        "            base_url = \"https://www.politifact.com\"\n",
        "            if link_tag and 'href' in link_tag.attrs:\n",
        "                relative_url = link_tag['href']\n",
        "                detail_url = base_url + relative_url\n",
        "            else:\n",
        "                detail_url = \"Detail URL not available\"\n",
        "\n",
        "            # Append the extracted data\n",
        "            fact_checks.append({\n",
        "                'Claim': claim,\n",
        "                'Detail_URL': detail_url,\n",
        "                'Threads_Post_URL': None  # Placeholder for Threads post URL\n",
        "            })\n",
        "    return fact_checks\n",
        "\n",
        "def scrape_threads_post_url(detail_url):\n",
        "    \"\"\"Extract the Threads post URL from the 'Our Sources' section of the detailed page.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(detail_url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Locate the \"Our Sources\" section\n",
        "        sources_section = soup.find('section', id='sources', class_='m-superbox')\n",
        "        article_content = sources_section.find('article', class_='m-superbox__content') if sources_section else None\n",
        "\n",
        "        if article_content:\n",
        "            first_p = article_content.find('p')\n",
        "            if first_p:\n",
        "                a_tag = first_p.find('a')  # Find the first <a> tag inside the <p>\n",
        "                if a_tag and 'href' in a_tag.attrs:\n",
        "                    return a_tag['href']  # Return the extracted URL\n",
        "        return \"Threads Post URL not found\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {detail_url}: {e}\")\n",
        "        return \"Error extracting Threads Post URL\"\n",
        "\n",
        "# Save data to a CSV file\n",
        "def save_to_csv(fact_checks, filename=\"fact_checks.csv\"):\n",
        "    keys = ['Claim', 'Detail_URL', 'Threads_Post_URL']\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(fact_checks)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Main script\n",
        "def main():\n",
        "    # Step 1: Scrape the main list of fact checks from pages 1 to 5\n",
        "    fact_checks = scrape_fact_checks(page_start=1, page_end=5)\n",
        "\n",
        "    # Step 2: Scrape Threads post URLs from each detailed page\n",
        "    for fact_check in fact_checks:\n",
        "        detail_url = fact_check['Detail_URL']\n",
        "        if detail_url and \"Detail URL not available\" not in detail_url:\n",
        "            print(f\"Scraping Threads post URL from: {detail_url}\")\n",
        "            threads_post_url = scrape_threads_post_url(detail_url)\n",
        "            fact_check['Threads_Post_URL'] = threads_post_url\n",
        "            time.sleep(1)  # Add delay to avoid overloading the server\n",
        "        else:\n",
        "            fact_check['Threads_Post_URL'] = \"Detail URL not available\"\n",
        "\n",
        "    # Step 3: Save the data to a CSV file\n",
        "    save_to_csv(fact_checks)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zK2t62LnlYc",
        "outputId": "fbdfb384-a3a4-45f2-b745-d451f6f683b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1: https://www.politifact.com/factchecks/list/?page=1&speaker=threads-posts\n",
            "Scraping page 2: https://www.politifact.com/factchecks/list/?page=2&speaker=threads-posts\n",
            "Scraping page 3: https://www.politifact.com/factchecks/list/?page=3&speaker=threads-posts\n",
            "Scraping page 4: https://www.politifact.com/factchecks/list/?page=4&speaker=threads-posts\n",
            "Scraping page 5: https://www.politifact.com/factchecks/list/?page=5&speaker=threads-posts\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/21/threads-posts/this-supposed-associated-press-headline-that-missp/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/20/threads-posts/proposed-bill-banning-stock-trading-wouldnt-bar-tr/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/20/threads-posts/the-proof-is-in-the-pudding-elon-musk-isnt-suspend/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/19/threads-posts/no-shrek-on-x-claims-that-musk-banned-dreamworks-i/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/18/threads-posts/no-taylor-swift-didnt-say-this-quote-about-elon-mu/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/18/threads-posts/elon-musk-is-rich-but-not-rich-enough-to-give-ever/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/18/threads-posts/social-media-posts-spread-false-claims-about-benja/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/15/threads-posts/no-supermarket-companies-kroger-food-lion-and-publ/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/15/threads-posts/no-president-joe-biden-didnt-agree-to-a-recount-in/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/14/threads-posts/nissans-planned-job-cuts-not-related-to-potential/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/14/threads-posts/claims-that-sephora-donated-to-the-trump-campaign/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/14/threads-posts/ella-emhoff-kamala-harris-stepdaughter-was-not-adm/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/13/threads-posts/have-disney-marvel-and-warner-bros-left-x-no-theyr/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/13/threads-posts/special-counsel-jack-smith-had-not-been-fired-as-o/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/13/threads-posts/biden-and-trump-werent-the-only-two-presidential-c/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/13/threads-posts/no-nato-secretary-general-mark-rutte-didnt-say-hed/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/12/threads-posts/2015-marco-rubio-video-isnt-evidence-donald-trump/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/12/threads-posts/no-cbs-news-didnt-report-on-cheating-in-the-electi/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/12/threads-posts/seeing-red-no-taylor-swift-didnt-cancel-eras-tour/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/12/threads-posts/no-arizona-election-officials-werent-caught-and-ja/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/12/threads-posts/no-elon-musks-starlink-wasnt-used-to-rig-the-2024/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/11/threads-posts/no-california-gov-gavin-newsom-isnt-considering-se/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/11/threads-posts/abortion-never-medically-necessary-late-in-pregnan/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/08/threads-posts/nevadas-ballot-curing-process-fuels-misleading-cla/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/08/threads-posts/no-nancy-pelosi-didnt-promise-to-impeach-donald-tr/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/08/threads-posts/shaky-social-media-video-doesnt-show-80-magnitude/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/08/threads-posts/no-kamala-harris-wouldnt-have-won-wisconsin-with-j/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/08/threads-posts/dont-fall-for-it-claim-that-stephen-king-was-banne/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/07/threads-posts/no-20-million-democratic-votes-didnt-disappear-and/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/07/threads-posts/millions-of-democratic-votes-were-sent-into-a-blac/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/07/threads-posts/viral-claim-that-riley-gaines-called-whoopi-goldbe/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/06/threads-posts/shark-bait-viral-claim-that-mark-cuban-lost-1-bill/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/06/threads-posts/no-hay-evidencia-de-que-el-nombre-de-donald-trump/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/06/threads-posts/no-melania-trump-didnt-endorse-kamala-harris-over/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/05/threads-posts/no-a-melania-trump-impostor-didnt-accompany-donald/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/05/threads-posts/no-jamie-raskin-didnt-say-he-wouldnt-certify-the-e/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/05/threads-posts/no-this-isnt-an-image-of-joe-biden-hanging-from-a/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/04/threads-posts/las-afirmaciones-de-que-elon-musk-esta-desarrollan/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/04/threads-posts/no-liz-cheney-didnt-say-trump-has-a-daffy-duck-dis/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/04/threads-posts/no-trump-didnt-explain-viral-microphone-stand-mome/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/03/threads-posts/joe-rogan-hasnt-said-as-of-nov-3-who-hes-voting-fo/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/nov/01/threads-posts/no-the-trump-campaign-is-not-selling-signs-with-sw/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/31/threads-posts/donald-trump-campaigned-at-a-mcdonalds-in-pennsylv/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/31/threads-posts/harris-hasnt-appeared-on-the-joe-rogan-experience/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/30/threads-posts/claim-about-aoc-being-worth-29-million-is-still-pa/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/30/threads-posts/video-shows-a-brazilian-demonstration-not-puerto-r/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/30/threads-posts/no-beyonce-and-jay-z-have-not-been-arrested-in-sea/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/28/threads-posts/screenshot-shows-four-florida-counties-zero-early/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/25/threads-posts/kamala-harris-comments-about-donald-trump-and-hitl/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/25/threads-posts/no-clear-winner-on-election-night-thats-not-eviden/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/25/threads-posts/no-immigrants-who-entered-the-us-under-bidens-admi/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/24/threads-posts/story-about-elon-musk-buying-cnn-originated-on-a-s/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/23/threads-posts/no-gov-ron-desantis-didnt-say-dominion-voting-syst/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/22/threads-posts/jilted-out-of-free-fries-no-trump-didnt-offer-to-b/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/22/threads-posts/a-load-of-misinformation-former-president-donald-t/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/22/threads-posts/no-megyn-kelly-didnt-call-for-a-taylor-swift-boyco/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/22/threads-posts/no-this-video-doesnt-show-karl-rove-stumping-for-k/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/21/threads-posts/did-trump-say-i-dont-poop-myself-on-truth-social-n/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/18/threads-posts/northern-lights-arent-chemicals-sprayed-in-the-sky/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/18/threads-posts/no-donald-trumps-felony-conviction-hasnt-been-over/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/16/threads-posts/no-journalist-bob-woodward-did-not-post-on-x-about/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/13/threads-posts/claims-that-kid-rock-aided-storm-victims-while-tay/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/11/threads-posts/tim-walz-wasnt-banned-from-college-football-games/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/11/threads-posts/flooding-footage-shows-mumbai-india-not-florida-du/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/10/threads-posts/femas-website-doesnt-say-it-must-help-white-people/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/09/threads-posts/americans-in-lebanon-and-israel-had-to-pay-for-the/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/oct/02/threads-posts/no-this-isnt-a-real-hurricane-helene-meteorologist/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/27/threads-posts/old-unfounded-claims-that-melania-trump-was-an-esc/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/26/threads-posts/no-this-isnt-an-authentic-truth-social-post-from-d/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/25/threads-posts/no-us-marshals-dont-escort-oprah-winfrey-around/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/25/threads-posts/no-democratic-sen-john-fetterman-didnt-say-hes-vot/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/25/threads-posts/its-tim-not-tom-walz-but-typo-on-florida-countys-o/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/24/threads-posts/no-travis-kelce-didnt-endorse-kamala-harris-for-pr/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/19/threads-posts/ready-for-it-taylor-swift-didnt-lose-millions-of-f/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/18/threads-posts/taylor-swift-remains-mum-on-elon-musks-impregnatio/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/18/threads-posts/old-sean-diddy-combs-mugshot-recirculates-after-re/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/18/threads-posts/a-donald-trump-fan-account-not-the-former-presiden/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/15/threads-posts/claim-that-abc-lost-27-million-in-ad-revenue-after/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/13/threads-posts/no-taylor-swifts-boyfriend-travis-kelce-didnt-say/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/04/threads-posts/photo-of-reality-show-stars-doesnt-feature-vice-pr/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/sep/04/threads-posts/no-melania-trump-didnt-file-for-divorce-in-palm-be/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/29/threads-posts/fake-x-post-shared-to-falsely-claim-charlie-kirk-r/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/28/threads-posts/no-tim-walz-did-not-insult-ann-coulter-after-she-c/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/27/threads-posts/no-donald-trump-didnt-post-that-kamala-harris-used/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/26/threads-posts/no-lowes-ceo-did-not-tell-conservatives-to-take-th/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/22/threads-posts/anybody-out-there-the-democratic-national-conventi/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/21/threads-posts/no-trump-did-not-say-this-viral-quote-about-buying/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/14/threads-posts/trump-didnt-suddenly-freeze-he-was-reacting-to-a-m/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/09/threads-posts/mike-tyson-didnt-say-hed-box-algerian-imane-khelif/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/09/threads-posts/this-photo-of-hilter-was-altered-to-resemble-a-tru/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/aug/01/threads-posts/kamala-harris-was-born-in-the-us-not-canada/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/31/threads-posts/switzerland-didnt-disacknowledge-islam-as-social-m/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/29/threads-posts/no-kamala-harris-didnt-say-this-quote-about-the-ol/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/25/threads-posts/social-media-rumors-claim-without-evidence-that-ta/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/24/threads-posts/no-usha-vance-didnt-call-kamala-harris-a-dei-hire/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/24/threads-posts/no-trump-isnt-backing-down-from-a-debate-with-harr/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/22/threads-posts/donald-trump-once-donated-to-kamala-harris-campaig/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/17/threads-posts/claims-that-the-secret-service-staged-trump-rally/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/16/threads-posts/does-a-photo-show-that-trumps-ear-grew-back-after/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/16/threads-posts/claim-that-thomas-matthew-crooks-was-arrested-more/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/15/threads-posts/no-proof-instagram-account-with-epstein-related-bi/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/15/threads-posts/no-this-photo-doesnt-show-barron-trump-in-the-crow/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/15/threads-posts/image-that-appears-to-show-secret-service-agents-s/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/14/threads-posts/donald-trump-shooter-was-not-identified-as-32-year/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/12/threads-posts/project-2025-doesnt-call-for-eliminating-osha-or-o/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/12/threads-posts/no-proof-donald-trump-made-settlements-to-10-to-13/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/09/threads-posts/space-rocks-but-the-meteor-in-this-video-in-hungar/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/09/threads-posts/no-a-july-3-cnn-report-did-not-say-joe-biden-dropp/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/09/threads-posts/new-york-times-threads-account-briefly-closed-but/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/08/threads-posts/ap-didnt-report-prosecutors-are-reconsidering-rape/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/08/threads-posts/no-project-2025-didnt-call-for-period-passports-fo/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/05/threads-posts/no-c-sections-are-not-coded-as-abortions-in-hospit/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jul/03/threads-posts/no-el-huracan-beryl-no-paso-por-las-islas-canarias/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/26/threads-posts/we-feel-fine-cern-scientists-did-not-end-the-world/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/25/threads-posts/no-brittney-griner-didnt-ask-angel-reese-to-join-a/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/24/threads-posts/men-aged-between-18-and-26-have-not-been-automatic/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/21/threads-posts/no-caitlin-clark-didnt-join-canadas-olympic-team/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/21/threads-posts/social-media-post-takes-pfizer-ceos-covid-19-rehea/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/21/threads-posts/caitlin-clark-didnt-replace-brittney-griner-on-the/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/21/threads-posts/claims-that-linguist-noam-chomsky-is-dead-are-not/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/17/threads-posts/apple-has-not-announced-plans-to-charge-facetime-u/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/14/threads-posts/the-false-claim-that-trumps-felony-conviction-bars/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/14/threads-posts/tale-of-pride-flag-burner-destroying-house-is-fict/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/06/threads-posts/a-fabricated-social-media-post-ensnares-dragons-di/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/05/threads-posts/no-former-president-donald-trump-didnt-blow-off-hi/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/jun/05/threads-posts/kyle-rittenhouses-mother-is-49-not-38-as-some-soci/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/may/17/threads-posts/chiefs-harrison-butker-criticized-for-graduation-s/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/may/15/threads-posts/a-south-carolina-hiker-saw-a-huge-snake-in-2021-bu/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/may/15/threads-posts/this-image-doesnt-show-timothy-olyphant-holding-a/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/may/13/threads-posts/no-former-president-donald-trumps-truth-social-did/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/may/03/threads-posts/trump-wants-an-iron-dome-but-he-didnt-say-to-defen/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/apr/26/threads-posts/no-cnn-didnt-report-that-donald-trump-soiled-himse/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/apr/24/threads-posts/trump-attended-his-eldest-childrens-graduations-co/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/apr/19/threads-posts/former-president-donald-trump-attended-ivanka-trum/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/apr/15/threads-posts/no-president-joe-biden-didnt-address-nation-from-o/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2024/apr/15/threads-posts/most-of-irans-missiles-were-intercepted-but-some-r/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2023/jul/13/threads-posts/no-no-esta-oculto-el-numero-de-la-bestia-666-en-el/\n",
            "Scraping Threads post URL from: https://www.politifact.com/factchecks/2023/jul/06/threads-posts/the-threads-app-is-here-and-no-elon-musk-did-not-t/\n",
            "Data saved to fact_checks.csv\n"
          ]
        }
      ],
      "id": "5zK2t62LnlYc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Username"
      ],
      "metadata": {
        "id": "iKpE3-vMA2ck"
      },
      "id": "iKpE3-vMA2ck"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "file_path = \"/content/fact_checks.csv\"  # Replace with your CSV file's path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check if the 'Threads_Post_URL' column exists\n",
        "if 'Threads_Post_URL' in df.columns:\n",
        "    # Extract username from the Threads_Post_URL column\n",
        "    df['Username'] = df['Threads_Post_URL'].str.extract(r'(@\\w+)')\n",
        "    print(df)\n",
        "\n",
        "    # Save the updated dataset to a new CSV file\n",
        "    updated_file_path = \"updated_fact_checks.csv\"\n",
        "    df.to_csv(updated_file_path, index=False)\n",
        "    print(f\"Updated dataset saved to {updated_file_path}\")\n",
        "else:\n",
        "    print(\"The 'Threads_Post_URL' column does not exist in the provided file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU_0g-ZxkFQW",
        "outputId": "75d9c688-f461-48d8-8514-12d159de86d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Claim  \\\n",
            "0    The Associated Press misspelled Mississippi in...   \n",
            "1    “The U.S Senate accidentally passed a bill ban...   \n",
            "2    “Elon Musk thinks that posting what you eat is...   \n",
            "3    “Elon Musk bans DreamWorks Animation from X fo...   \n",
            "4    Says Taylor Swift said, “Elon Musk, the first ...   \n",
            "..                                                 ...   \n",
            "133  Former President Donald Trump didn’t attend da...   \n",
            "134  President Joe Biden “will address the nation f...   \n",
            "135      “Not even one rocket (from Iran) hit Israel.”   \n",
            "136  El logo de Threads oculta “el número de la bes...   \n",
            "137  Elon Musk tweeted that he “just downloaded Thr...   \n",
            "\n",
            "                                            Detail_URL  \\\n",
            "0    https://www.politifact.com/factchecks/2024/nov...   \n",
            "1    https://www.politifact.com/factchecks/2024/nov...   \n",
            "2    https://www.politifact.com/factchecks/2024/nov...   \n",
            "3    https://www.politifact.com/factchecks/2024/nov...   \n",
            "4    https://www.politifact.com/factchecks/2024/nov...   \n",
            "..                                                 ...   \n",
            "133  https://www.politifact.com/factchecks/2024/apr...   \n",
            "134  https://www.politifact.com/factchecks/2024/apr...   \n",
            "135  https://www.politifact.com/factchecks/2024/apr...   \n",
            "136  https://www.politifact.com/factchecks/2023/jul...   \n",
            "137  https://www.politifact.com/factchecks/2023/jul...   \n",
            "\n",
            "                                      Threads_Post_URL           Username  \n",
            "0    https://www.threads.net/@thompsonphil60/post/D...    @thompsonphil60  \n",
            "1    https://www.threads.net/@allhallowswitchh/post...  @allhallowswitchh  \n",
            "2    https://www.threads.net/@serenablue2024/post/D...    @serenablue2024  \n",
            "3    https://www.threads.net/@currentusupdates/post...  @currentusupdates  \n",
            "4               https://ghostarchive.org/archive/vAD9e                NaN  \n",
            "..                                                 ...                ...  \n",
            "133                           https://archive.is/WdwDA                NaN  \n",
            "134  https://www.threads.net/@camarrero/post/C5t8CA...         @camarrero  \n",
            "135  https://www.threads.net/@talzsilverstone/post/...   @talzsilverstone  \n",
            "136  https://www.biblegateway.com/passage/?search=A...                NaN  \n",
            "137              https://www.threads.net/t/CuV0aUnLUId                NaN  \n",
            "\n",
            "[138 rows x 4 columns]\n",
            "Updated dataset saved to updated_fact_checks.csv\n"
          ]
        }
      ],
      "id": "WU_0g-ZxkFQW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Codes"
      ],
      "metadata": {
        "id": "T8kRykEiKa59"
      },
      "id": "T8kRykEiKa59"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the input CSV file\n",
        "df = pd.read_csv(\"/content/updated_fact_checks all 5 pages.csv\")\n",
        "\n",
        "# Function to extract the code from a Threads URL\n",
        "def extract_code(Threads_Post_URL):\n",
        "    if pd.notnull(Threads_Post_URL):  # Check if the URL is not null\n",
        "        match = re.search(r'/post/([a-zA-Z0-9_-]+)', Threads_Post_URL)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Apply the function to the Threads URL column and create a new Code column\n",
        "df['Code'] = df['Threads_Post_URL'].apply(extract_code)\n",
        "\n",
        "# Preview the processed data\n",
        "print(\"Processed data (first 10 rows):\")\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ3hmfo1KZ8U",
        "outputId": "d281c57b-a114-44b0-853f-9bcacf4672e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data (first 10 rows):\n",
            "                                               Claim  \\\n",
            "0  The Associated Press misspelled Mississippi in...   \n",
            "1  “The U.S Senate accidentally passed a bill ban...   \n",
            "2  “Elon Musk thinks that posting what you eat is...   \n",
            "3  “Elon Musk bans DreamWorks Animation from X fo...   \n",
            "4  Says Taylor Swift said, “Elon Musk, the first ...   \n",
            "5  “Elon Musk has $180 Billion and there are 8B p...   \n",
            "6  Israeli Prime Minister Benjamin Netanyahu hosp...   \n",
            "7  “Kroger, Food Lion and Publix announce a 4% pr...   \n",
            "8  “Biden agreed to a recount” in the 2024 presid...   \n",
            "9  Nissan just announced 9,000 layoffs in Tenness...   \n",
            "\n",
            "                                          Detail_URL  \\\n",
            "0  https://www.politifact.com/factchecks/2024/nov...   \n",
            "1  https://www.politifact.com/factchecks/2024/nov...   \n",
            "2  https://www.politifact.com/factchecks/2024/nov...   \n",
            "3  https://www.politifact.com/factchecks/2024/nov...   \n",
            "4  https://www.politifact.com/factchecks/2024/nov...   \n",
            "5  https://www.politifact.com/factchecks/2024/nov...   \n",
            "6  https://www.politifact.com/factchecks/2024/nov...   \n",
            "7  https://www.politifact.com/factchecks/2024/nov...   \n",
            "8  https://www.politifact.com/factchecks/2024/nov...   \n",
            "9  https://www.politifact.com/factchecks/2024/nov...   \n",
            "\n",
            "                                    Threads_Post_URL             Username  \\\n",
            "0  https://www.threads.net/@thompsonphil60/post/D...      @thompsonphil60   \n",
            "1  https://www.threads.net/@allhallowswitchh/post...    @allhallowswitchh   \n",
            "2  https://www.threads.net/@serenablue2024/post/D...      @serenablue2024   \n",
            "3  https://www.threads.net/@currentusupdates/post...    @currentusupdates   \n",
            "4             https://ghostarchive.org/archive/vAD9e                  NaN   \n",
            "5  https://www.threads.net/@briancharlesrooney/po...  @briancharlesrooney   \n",
            "6  https://www.threads.net/@nya360_/post/DCfF-PpyT6w             @nya360_   \n",
            "7  https://www.threads.net/@stocks.tom/post/DCOwW...              @stocks   \n",
            "8  https://www.threads.net/@steelergal1215/post/D...      @steelergal1215   \n",
            "9  https://www.threads.net/@profrankmueller/post/...     @profrankmueller   \n",
            "\n",
            "          Code  \n",
            "0  DCgt36GttDq  \n",
            "1  DCkyrbCvuvZ  \n",
            "2  DCegZjZxQPp  \n",
            "3  DCfqV61sbcA  \n",
            "4         None  \n",
            "5  DCZKW8RP_eN  \n",
            "6  DCfF-PpyT6w  \n",
            "7  DCOwW1XxrBr  \n",
            "8  DCLFFL0uVaZ  \n",
            "9  DCT_S_Nt87w  \n"
          ]
        }
      ],
      "id": "wZ3hmfo1KZ8U"
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"Politifact_Extracted_Codes.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Data saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGfQSZKfLn6B",
        "outputId": "0a418b27-60ab-42df-8c03-54d6cd3f70d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Politifact_Extracted_Codes.csv\n"
          ]
        }
      ],
      "id": "DGfQSZKfLn6B"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNfpn4_JzKOf"
      },
      "id": "zNfpn4_JzKOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lead Stories Data Scraping**\n",
        "\n"
      ],
      "metadata": {
        "id": "faJqBe-xHDBu"
      },
      "id": "faJqBe-xHDBu"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL format\n",
        "base_url = \"https://leadstories.com/cgi-bin/mt/mt-search.fcgi?search=threads&IncludeBlogs=1&blog_id=1&limit=10&page={}\"\n",
        "\n",
        "data = []\n",
        "\n",
        "# Loop through all 13 pages\n",
        "for page in range(1, 14):  # Page numbers from 1 to 13\n",
        "    url = base_url.format(page)\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract article headlines and links\n",
        "    articles = soup.find_all('a', class_='overlay-link grid')  # Adjust class name if needed\n",
        "\n",
        "    for article in articles:\n",
        "        article_url = article['href']\n",
        "        headline = article.find('h2', class_=\"mod-default-article-title\").text.strip()\n",
        "\n",
        "        # Visit each article page\n",
        "        article_response = requests.get(article_url)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "        # Extract the Threads URL (assuming it's in the second paragraph)\n",
        "        paragraphs = article_soup.find_all('p')\n",
        "        if len(paragraphs) > 1:\n",
        "            threads_url = paragraphs[1].find('a')['href'] if paragraphs[1].find('a') else None\n",
        "        else:\n",
        "            threads_url = None\n",
        "\n",
        "        # Append data\n",
        "        data.append({\n",
        "            'Headline': headline,\n",
        "            'Article Url': article_url,\n",
        "            'Threads URL': threads_url\n",
        "        })\n",
        "\n",
        "    print(f\"Page {page} scraped successfully.\")\n",
        "\n",
        "# Print a preview of the data\n",
        "print(\"Sample data:\")\n",
        "for item in data[:10]:  # Display the first 10 entries\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF1hoTjESbN2",
        "outputId": "69c4c583-e098-4640-85c7-09ad323aec0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 scraped successfully.\n",
            "Page 2 scraped successfully.\n",
            "Page 3 scraped successfully.\n",
            "Page 4 scraped successfully.\n",
            "Page 5 scraped successfully.\n",
            "Page 6 scraped successfully.\n",
            "Page 7 scraped successfully.\n",
            "Page 8 scraped successfully.\n",
            "Page 9 scraped successfully.\n",
            "Page 10 scraped successfully.\n",
            "Page 11 scraped successfully.\n",
            "Page 12 scraped successfully.\n",
            "Page 13 scraped successfully.\n",
            "Sample data:\n",
            "{'Headline': 'Fact Check: Image Of Elon Musk In Profile Is NOT Authentic -- Edited To Make Belly Bigger And Rear Smaller', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-image-of-elon-musk-is-not-authentic-edited-to-make-belly-bigger-and-rear-smaller.html', 'Threads URL': 'https://www.threads.net/@mediumsizemeech/post/DCIa3XjI4bo/breaking-elon-musk-pregnant-at-53'}\n",
            "{'Headline': 'Fact Check: Mexican President Did NOT Say Trump Tariff Threat Is Why Migrant Caravans No Longer At Border', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-mexican-president-did-not-say-trump-tariff-threat-is-why-migrant-caravans-no-longer-at-border.html', 'Threads URL': 'https://www.threads.net/@charliekirk1776/post/DC2eoEaybrH'}\n",
            "{'Headline': 'Fact Check: Law Banning TikTok in U.S. After January 19, 2025 DOES Have Option For Deadline Extension -- Lawsuits Ongoing, New Admin Incoming', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-it-is-not-certain-tiktok-will-be-shut-down-january-19-2025.html', 'Threads URL': 'https://www.threads.net/@manuborrero/post/DCaNYwiSN4K'}\n",
            "{'Headline': \"Fact Check: NO Evidence Taylor Swift Announced She'll Move To Canada In Response To 2024 Presidential Election Results\", 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-no-proof-taylor-swift-moved-to-canada-in-response-to-2024-presidential-election-results.html', 'Threads URL': 'https://www.facebook.com/thingsineeed/posts/pfbid02KXKuAjAyefDR7mi6T657KKVh8Ngy9q4NyuZu1qY9TLxMdjqnVEF5i8CJBQryJak2l'}\n",
            "{'Headline': \"Fact Check: Taylor Swift Did NOT Say Elon Musk Is 'First Person In History, To Be Radicalized By His Own Algorithm'\", 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-taylor-swift-did-not-say-elon-musk-is-first-person-in-history-to-be-radicalized-by-own-algorithm.html', 'Threads URL': 'https://www.threads.net/@spursgirlie/post/DCaE31MpGKp'}\n",
            "{'Headline': 'Fact Check: Texas Did NOT Gift Trump 355,000 Acres For Deportation Camp -- But Texas Official Did Propose Use Of Ranch', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-texas-did-not-gift-trump-three-hundred-fifty-five-thousand-acres-for-deportation-camp.html', 'Threads URL': 'https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr'}\n",
            "{'Headline': 'Fact Check: Image Of Netanyahu On Ventilator In Hospital Is NOT Authentic', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-image-of-netanyahu-on-ventilator-in-hospital-is-not-authentic.html', 'Threads URL': 'https://www.threads.net/@nya360_/post/DCfF-PpyT6w'}\n",
            "{'Headline': 'Fact Check: ETHICS Act Would NOT Prevent Trump From Becoming President In 2025', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-ethics-act-would-not-prevent-trump-from-becoming-president-in-2025.html', 'Threads URL': 'https://x.com/politvidchannel/status/1858859762395558286'}\n",
            "{'Headline': \"Fact Check: NO Evidence That Elon Musk Said He Would Suspend Accounts That Called Him 'Fat,' 'The First Lady'\", 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-no-evidence-that-elon-musk-said-he-would-suspend-accounts-that-called-him-fat-the-first-lady.html', 'Threads URL': 'https://www.threads.net/@minds.nexus/post/DCmN1B8RDEK'}\n",
            "{'Headline': 'Fact Check: Elon Musk Could NOT Give Everyone On Earth $1 Billion And Still Have $172 Billion Left Over', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-elon-musk-could-not-give-everyone-on-earth-1-billion-and-still-have-172-billion-left-over.html', 'Threads URL': 'https://www.threads.net/@briancharlesrooney/post/DCZKW8RP_eN?fbclid=IwY2xjawGr5qFleHRuA2FlbQIxMAABHS_fquy5W7Gl6ZDhjn9CWRbFGu1mrGRxj6nJBSMh4rNQx9WvQCLvfb170w_aem_EE9giYZQoUx7tkZzPAVY_A'}\n"
          ]
        }
      ],
      "id": "CF1hoTjESbN2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data to a CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('LeadStoriesDatabase.csv', index=False)\n",
        "print(\"Data saved to threads_articles_all_pages.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iToAur1lTE3P",
        "outputId": "06b1e39c-8c72-43b5-d070-461430d62b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to threads_articles_all_pages.csv\n"
          ]
        }
      ],
      "id": "iToAur1lTE3P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting username"
      ],
      "metadata": {
        "id": "sCj707caAiBm"
      },
      "id": "sCj707caAiBm"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL format\n",
        "base_url = \"https://leadstories.com/cgi-bin/mt/mt-search.fcgi?search=threads&IncludeBlogs=1&blog_id=1&limit=10&page={}\"\n",
        "\n",
        "data = []\n",
        "\n",
        "# Loop through all 13 pages\n",
        "for page in range(1, 14):  # Page numbers from 1 to 13\n",
        "    url = base_url.format(page)\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract article headlines and links\n",
        "    articles = soup.find_all('a', class_='overlay-link grid')  # Adjust class name if needed\n",
        "\n",
        "    for article in articles:\n",
        "        article_url = article['href']\n",
        "        headline = article.find('h2', class_=\"mod-default-article-title\").text.strip()\n",
        "\n",
        "        # Visit each article page\n",
        "        article_response = requests.get(article_url)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "        # Extract the Threads URL (assuming it's in the second paragraph)\n",
        "        paragraphs = article_soup.find_all('p')\n",
        "        if len(paragraphs) > 1:\n",
        "            threads_url = paragraphs[1].find('a')['href'] if paragraphs[1].find('a') else None\n",
        "        else:\n",
        "            threads_url = None\n",
        "\n",
        "        # Extract username if Threads URL exists\n",
        "        username = None\n",
        "        if threads_url and \"threads.net\" in threads_url:\n",
        "            match = re.search(r'@([a-zA-Z0-9_.]+)', threads_url)  # Extract username\n",
        "            if match:\n",
        "                username = f\"@{match.group(1)}\"\n",
        "\n",
        "        # Append data\n",
        "        data.append({\n",
        "            'Headline': headline,\n",
        "            'Article Url': article_url,\n",
        "            'Threads URL': threads_url,\n",
        "            'Username': username\n",
        "        })\n",
        "\n",
        "    print(f\"Page {page} scraped successfully.\")\n",
        "\n",
        "# Print a preview of the data\n",
        "print(\"Sample data:\")\n",
        "for item in data[:10]:  # Display the first 10 entries\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t__hp3RW6-y",
        "outputId": "9e2609a4-8e55-469e-c01f-686e0c65a497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 scraped successfully.\n",
            "Page 2 scraped successfully.\n",
            "Page 3 scraped successfully.\n",
            "Page 4 scraped successfully.\n",
            "Page 5 scraped successfully.\n",
            "Page 6 scraped successfully.\n",
            "Page 7 scraped successfully.\n",
            "Page 8 scraped successfully.\n",
            "Page 9 scraped successfully.\n",
            "Page 10 scraped successfully.\n",
            "Page 11 scraped successfully.\n",
            "Page 12 scraped successfully.\n",
            "Page 13 scraped successfully.\n",
            "Sample data:\n",
            "{'Headline': 'Fact Check: Image Of Elon Musk In Profile Is NOT Authentic -- Edited To Make Belly Bigger And Rear Smaller', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-image-of-elon-musk-is-not-authentic-edited-to-make-belly-bigger-and-rear-smaller.html', 'Threads URL': 'https://www.threads.net/@mediumsizemeech/post/DCIa3XjI4bo/breaking-elon-musk-pregnant-at-53', 'Username': '@mediumsizemeech'}\n",
            "{'Headline': 'Fact Check: Mexican President Did NOT Say Trump Tariff Threat Is Why Migrant Caravans No Longer At Border', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-mexican-president-did-not-say-trump-tariff-threat-is-why-migrant-caravans-no-longer-at-border.html', 'Threads URL': 'https://www.threads.net/@charliekirk1776/post/DC2eoEaybrH', 'Username': '@charliekirk1776'}\n",
            "{'Headline': 'Fact Check: Law Banning TikTok in U.S. After January 19, 2025 DOES Have Option For Deadline Extension -- Lawsuits Ongoing, New Admin Incoming', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-it-is-not-certain-tiktok-will-be-shut-down-january-19-2025.html', 'Threads URL': 'https://www.threads.net/@manuborrero/post/DCaNYwiSN4K', 'Username': '@manuborrero'}\n",
            "{'Headline': \"Fact Check: NO Evidence Taylor Swift Announced She'll Move To Canada In Response To 2024 Presidential Election Results\", 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-no-proof-taylor-swift-moved-to-canada-in-response-to-2024-presidential-election-results.html', 'Threads URL': 'https://www.facebook.com/thingsineeed/posts/pfbid02KXKuAjAyefDR7mi6T657KKVh8Ngy9q4NyuZu1qY9TLxMdjqnVEF5i8CJBQryJak2l', 'Username': None}\n",
            "{'Headline': \"Fact Check: Taylor Swift Did NOT Say Elon Musk Is 'First Person In History, To Be Radicalized By His Own Algorithm'\", 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-taylor-swift-did-not-say-elon-musk-is-first-person-in-history-to-be-radicalized-by-own-algorithm.html', 'Threads URL': 'https://www.threads.net/@spursgirlie/post/DCaE31MpGKp', 'Username': '@spursgirlie'}\n",
            "{'Headline': 'Fact Check: Texas Did NOT Gift Trump 355,000 Acres For Deportation Camp -- But Texas Official Did Propose Use Of Ranch', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-texas-did-not-gift-trump-three-hundred-fifty-five-thousand-acres-for-deportation-camp.html', 'Threads URL': 'https://www.threads.net/@syeedthedoyenne/post/DCnCtojyuKr', 'Username': '@syeedthedoyenne'}\n",
            "{'Headline': 'Fact Check: Image Of Netanyahu On Ventilator In Hospital Is NOT Authentic', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-image-of-netanyahu-on-ventilator-in-hospital-is-not-authentic.html', 'Threads URL': 'https://www.threads.net/@nya360_/post/DCfF-PpyT6w', 'Username': '@nya360_'}\n",
            "{'Headline': 'Fact Check: ETHICS Act Would NOT Prevent Trump From Becoming President In 2025', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-ethics-act-would-not-prevent-trump-from-becoming-president-in-2025.html', 'Threads URL': 'https://x.com/politvidchannel/status/1858859762395558286', 'Username': None}\n",
            "{'Headline': \"Fact Check: NO Evidence That Elon Musk Said He Would Suspend Accounts That Called Him 'Fat,' 'The First Lady'\", 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-no-evidence-that-elon-musk-said-he-would-suspend-accounts-that-called-him-fat-the-first-lady.html', 'Threads URL': 'https://www.threads.net/@minds.nexus/post/DCmN1B8RDEK', 'Username': '@minds.nexus'}\n",
            "{'Headline': 'Fact Check: Elon Musk Could NOT Give Everyone On Earth $1 Billion And Still Have $172 Billion Left Over', 'Article Url': 'https://leadstories.com/hoax-alert/2024/11/fact-check-elon-musk-could-not-give-everyone-on-earth-1-billion-and-still-have-172-billion-left-over.html', 'Threads URL': 'https://www.threads.net/@briancharlesrooney/post/DCZKW8RP_eN?fbclid=IwY2xjawGr5qFleHRuA2FlbQIxMAABHS_fquy5W7Gl6ZDhjn9CWRbFGu1mrGRxj6nJBSMh4rNQx9WvQCLvfb170w_aem_EE9giYZQoUx7tkZzPAVY_A', 'Username': '@briancharlesrooney'}\n"
          ]
        }
      ],
      "id": "2t__hp3RW6-y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data to a CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('Updated_LeadStoriesDatabase.csv', index=False)\n",
        "print(\"Data saved to threads_articles_all_pages.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiCt3rdTYK14",
        "outputId": "c3fe1c36-2565-4dad-9c0e-77545fbd09b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to threads_articles_all_pages.csv\n"
          ]
        }
      ],
      "id": "FiCt3rdTYK14"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Codes"
      ],
      "metadata": {
        "id": "gXvK15BGMpR6"
      },
      "id": "gXvK15BGMpR6"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the input CSV file\n",
        "df = pd.read_csv(\"/content/Updated_LeadStoriesDatabase.csv\")\n",
        "\n",
        "# Function to extract the code from a Threads URL\n",
        "def extract_code(threads_url):\n",
        "    if pd.notnull(threads_url):  # Check if the URL is not null\n",
        "        match = re.search(r'/post/([a-zA-Z0-9_-]+)', threads_url)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None\n",
        "\n",
        "# Apply the function to the Threads URL column and create a new Code column\n",
        "df['Code'] = df['Threads URL'].apply(extract_code)\n",
        "\n",
        "# Preview the processed data\n",
        "print(\"Processed data (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gbyahMlFT_q",
        "outputId": "7ddd0969-6f97-4aee-a02b-764048388d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data (first 10 rows):\n",
            "                                            Headline  \\\n",
            "0  Fact Check: Image Of Elon Musk In Profile Is N...   \n",
            "1  Fact Check: Mexican President Did NOT Say Trum...   \n",
            "2  Fact Check: Law Banning TikTok in U.S. After J...   \n",
            "3  Fact Check: NO Evidence Taylor Swift Announced...   \n",
            "4  Fact Check: Taylor Swift Did NOT Say Elon Musk...   \n",
            "5  Fact Check: Texas Did NOT Gift Trump 355,000 A...   \n",
            "6  Fact Check: Image Of Netanyahu On Ventilator I...   \n",
            "7  Fact Check: ETHICS Act Would NOT Prevent Trump...   \n",
            "8  Fact Check: NO Evidence That Elon Musk Said He...   \n",
            "9  Fact Check: Elon Musk Could NOT Give Everyone ...   \n",
            "\n",
            "                                         Article Url  \\\n",
            "0  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "1  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "2  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "3  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "4  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "5  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "6  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "7  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "8  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "9  https://leadstories.com/hoax-alert/2024/11/fac...   \n",
            "\n",
            "                                         Threads URL             Username  \\\n",
            "0  https://www.threads.net/@mediumsizemeech/post/...     @mediumsizemeech   \n",
            "1  https://www.threads.net/@charliekirk1776/post/...     @charliekirk1776   \n",
            "2  https://www.threads.net/@manuborrero/post/DCaN...         @manuborrero   \n",
            "3  https://www.facebook.com/thingsineeed/posts/pf...                  NaN   \n",
            "4  https://www.threads.net/@spursgirlie/post/DCaE...         @spursgirlie   \n",
            "5  https://www.threads.net/@syeedthedoyenne/post/...     @syeedthedoyenne   \n",
            "6  https://www.threads.net/@nya360_/post/DCfF-PpyT6w             @nya360_   \n",
            "7  https://x.com/politvidchannel/status/185885976...                  NaN   \n",
            "8  https://www.threads.net/@minds.nexus/post/DCmN...         @minds.nexus   \n",
            "9  https://www.threads.net/@briancharlesrooney/po...  @briancharlesrooney   \n",
            "\n",
            "          Code  \n",
            "0  DCIa3XjI4bo  \n",
            "1  DC2eoEaybrH  \n",
            "2  DCaNYwiSN4K  \n",
            "3         None  \n",
            "4  DCaE31MpGKp  \n",
            "5  DCnCtojyuKr  \n",
            "6  DCfF-PpyT6w  \n",
            "7         None  \n",
            "8  DCmN1B8RDEK  \n",
            "9  DCZKW8RP_eN  \n"
          ]
        }
      ],
      "id": "3gbyahMlFT_q"
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"Leadstories_Extracted_Codes.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Data saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RneuczbGJCuk",
        "outputId": "3236dbb7-da9f-4537-cecb-0f52531433c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Leadstories_Extracted_Codes.csv\n"
          ]
        }
      ],
      "id": "RneuczbGJCuk"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFUSgKrjzPln"
      },
      "id": "vFUSgKrjzPln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **USA Today Data Scraping**\n",
        "\n"
      ],
      "metadata": {
        "id": "mAGIawfpJq8b"
      },
      "id": "mAGIawfpJq8b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scraping USAToday"
      ],
      "metadata": {
        "id": "HIt9f0bXJq8d"
      },
      "id": "HIt9f0bXJq8d"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Base URL for pagination\n",
        "base_url = \"https://www.usatoday.com/search/?q=threads&page=\"\n",
        "\n",
        "# List to store data\n",
        "data = []\n",
        "\n",
        "# Loop through the pages\n",
        "for page in range(1, 6):  # Adjust the range to the number of pages you want to scrape\n",
        "    print(f\"Scraping page {page}...\")\n",
        "    response = requests.get(base_url + str(page))\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract article headlines and links\n",
        "    articles = soup.find_all('a', class_='gnt_se_a gnt_se_a__hd gnt_se_a__hi')  # Adjust the class name based on the HTML structure\n",
        "\n",
        "    for article in articles:\n",
        "        headline = article.text.strip()\n",
        "        article_url = 'https://www.usatoday.com' + article['href']\n",
        "\n",
        "        # Step 2: Visit each article page\n",
        "        article_response = requests.get(article_url)\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "        # Extract Threads post URL\n",
        "        threads_url = None\n",
        "        for link in article_soup.find_all('a', href=True):  # Find all anchor tags with href\n",
        "            if \"threads.net\" in link['href'] and \"/post/\" in link['href']:  # Ensure it is a post URL\n",
        "                threads_url = link['href']\n",
        "                break  # Stop searching after finding the first valid post URL\n",
        "\n",
        "        # Append to data\n",
        "        data.append({\n",
        "            'Headline': headline,\n",
        "            'Article Url': article_url,\n",
        "            'Threads Post URL': threads_url\n",
        "        })\n",
        "\n",
        "# Convert data to a DataFrame for better visualization\n",
        "df = pd.DataFrame(data)\n",
        "df.head(10)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "# df.to_csv('usatoday_threads_data.csv', index=False)\n",
        "# print(\"Scraping completed. Data saved to 'usatoday_threads_data.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "bbbd50da-8d07-4e6e-8c17-44349de272e2",
        "id": "JNZ36d3JJq8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Headline  \\\n",
              "0  No, Canada: Fabricated Justin Trudeau post sho...   \n",
              "1  False claim Jimmy Carter, George H.W. Bush par...   \n",
              "2  False claim Georgia Supreme Court tossed case ...   \n",
              "3  QVC recalls more than a million pairs of Temp-...   \n",
              "4  Elon Musk became a US citizen in 2002, contrar...   \n",
              "5  Rare ‘snow doughnuts’ form in Michigan backyar...   \n",
              "6  Notre-Dame to reopen after brutal 2019 fire: S...   \n",
              "7  GM recalls over 132,000 Chevrolet, GMC trucks ...   \n",
              "8  'A special place': Chi-Chi's restaurants, popu...   \n",
              "9  Social media platform Bluesky nearing 25 milli...   \n",
              "\n",
              "                                         Article Url  \\\n",
              "0  https://www.usatoday.com/story/news/factcheck/...   \n",
              "1  https://www.usatoday.com/story/news/factcheck/...   \n",
              "2  https://www.usatoday.com/story/news/factcheck/...   \n",
              "3  https://www.usatoday.com/story/money/retail/20...   \n",
              "4  https://www.usatoday.com/story/news/factcheck/...   \n",
              "5  https://www.usatoday.com/story/news/weather/20...   \n",
              "6  https://www.usatoday.com/story/news/world/2024...   \n",
              "7  https://www.usatoday.com/story/money/cars/reca...   \n",
              "8  https://www.usatoday.com/story/money/food/2024...   \n",
              "9  https://www.usatoday.com/story/tech/2024/12/06...   \n",
              "\n",
              "                                    Threads Post URL  \n",
              "0  https://www.threads.net/@robertellingsworth/po...  \n",
              "1  https://www.threads.net/@indyeric10/post/DDFzi...  \n",
              "2  https://www.threads.net/@robertnagel_/post/DC-...  \n",
              "3                                               None  \n",
              "4  https://www.threads.net/@greenemachine1970/pos...  \n",
              "5                                               None  \n",
              "6                                               None  \n",
              "7                                               None  \n",
              "8                                               None  \n",
              "9                                               None  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7be2279-6180-42af-8af1-14cebd709931\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Article Url</th>\n",
              "      <th>Threads Post URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No, Canada: Fabricated Justin Trudeau post sho...</td>\n",
              "      <td>https://www.usatoday.com/story/news/factcheck/...</td>\n",
              "      <td>https://www.threads.net/@robertellingsworth/po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>False claim Jimmy Carter, George H.W. Bush par...</td>\n",
              "      <td>https://www.usatoday.com/story/news/factcheck/...</td>\n",
              "      <td>https://www.threads.net/@indyeric10/post/DDFzi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False claim Georgia Supreme Court tossed case ...</td>\n",
              "      <td>https://www.usatoday.com/story/news/factcheck/...</td>\n",
              "      <td>https://www.threads.net/@robertnagel_/post/DC-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>QVC recalls more than a million pairs of Temp-...</td>\n",
              "      <td>https://www.usatoday.com/story/money/retail/20...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Elon Musk became a US citizen in 2002, contrar...</td>\n",
              "      <td>https://www.usatoday.com/story/news/factcheck/...</td>\n",
              "      <td>https://www.threads.net/@greenemachine1970/pos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rare ‘snow doughnuts’ form in Michigan backyar...</td>\n",
              "      <td>https://www.usatoday.com/story/news/weather/20...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Notre-Dame to reopen after brutal 2019 fire: S...</td>\n",
              "      <td>https://www.usatoday.com/story/news/world/2024...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>GM recalls over 132,000 Chevrolet, GMC trucks ...</td>\n",
              "      <td>https://www.usatoday.com/story/money/cars/reca...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'A special place': Chi-Chi's restaurants, popu...</td>\n",
              "      <td>https://www.usatoday.com/story/money/food/2024...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Social media platform Bluesky nearing 25 milli...</td>\n",
              "      <td>https://www.usatoday.com/story/tech/2024/12/06...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7be2279-6180-42af-8af1-14cebd709931')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7be2279-6180-42af-8af1-14cebd709931 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7be2279-6180-42af-8af1-14cebd709931');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1f71b85d-b4b7-492b-88dd-7a889b7199ef\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f71b85d-b4b7-492b-88dd-7a889b7199ef')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1f71b85d-b4b7-492b-88dd-7a889b7199ef button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# print(\\\"Scraping completed\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Headline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"'A special place': Chi-Chi's restaurants, popular in the 90s, to open 20 years after closure\",\n          \"False claim Jimmy Carter, George H.W. Bush pardoned family members | Fact check\",\n          \"Rare \\u2018snow doughnuts\\u2019 form in Michigan backyard: See photos from week of wintry weather\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Article Url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"https://www.usatoday.com/story/money/food/2024/12/04/chi-chis-restaurants-opening-hormel-foods-mcdermott/76774868007/\",\n          \"https://www.usatoday.com/story/news/factcheck/2024/12/05/jimmy-carter-george-bush-pardon-relatives-fact-check/76762643007/\",\n          \"https://www.usatoday.com/story/news/weather/2024/12/06/winter-weather-us-snow-doughnuts-photos/76824035007/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Threads Post URL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"https://www.threads.net/@indyeric10/post/DDFzimKS4PF\",\n          \"https://www.threads.net/@greenemachine1970/post/DC8NNuSuY_Q\",\n          \"https://www.threads.net/@robertellingsworth/post/DDK-2l1vcN2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "id": "JNZ36d3JJq8d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data to a CSV file\n",
        "df.to_csv('usatoday_threads_data.csv', index=False)\n",
        "print(\"Scraping completed. Data saved to 'usatoday_threads_data.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b71df7-8407-4b68-9977-6be9a5d6e8bc",
        "id": "H6p0R0DdJq8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping completed. Data saved to 'usatoday_threads_data.csv'.\n"
          ]
        }
      ],
      "id": "H6p0R0DdJq8d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting username and codes"
      ],
      "metadata": {
        "id": "AbWR4gLsJq8d"
      },
      "id": "AbWR4gLsJq8d"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the input CSV file\n",
        "df = pd.read_csv(\"/content/usatoday_threads_data.csv\")\n",
        "\n",
        "# Function to extract both username and post code from a Threads URL\n",
        "def extract_username_and_code(threads_url):\n",
        "    if pd.notnull(threads_url):  # Check if the URL is not null\n",
        "        # Extract the username\n",
        "        username_match = re.search(r'@(\\w+)', threads_url)\n",
        "        # Extract the post code\n",
        "        code_match = re.search(r'/post/([a-zA-Z0-9_-]+)', threads_url)\n",
        "\n",
        "        username = username_match.group(0) if username_match else None\n",
        "        code = code_match.group(1) if code_match else None\n",
        "\n",
        "        return pd.Series([username, code])\n",
        "    return pd.Series([None, None])\n",
        "\n",
        "# Apply the function to the Threads URL column\n",
        "df[['Username', 'Code']] = df['Threads Post URL'].apply(extract_username_and_code)\n",
        "\n",
        "# Preview the processed data\n",
        "print(\"Processed data (first 10 rows):\")\n",
        "print(df.head(10))\n",
        "\n",
        "# Save the updated dataset to a new CSV file\n",
        "updated_file_path = \"usaToday_updated.csv\"\n",
        "df.to_csv(updated_file_path, index=False)\n",
        "print(f\"Updated dataset saved to {updated_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2938d758-37e1-46d5-8101-1e57cabd91d9",
        "id": "C454xskxJq8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data (first 10 rows):\n",
            "                                            Headline  \\\n",
            "0  No, Canada: Fabricated Justin Trudeau post sho...   \n",
            "1  False claim Jimmy Carter, George H.W. Bush par...   \n",
            "2  False claim Georgia Supreme Court tossed case ...   \n",
            "3  QVC recalls more than a million pairs of Temp-...   \n",
            "4  Elon Musk became a US citizen in 2002, contrar...   \n",
            "5  Rare ‘snow doughnuts’ form in Michigan backyar...   \n",
            "6  Notre-Dame to reopen after brutal 2019 fire: S...   \n",
            "7  GM recalls over 132,000 Chevrolet, GMC trucks ...   \n",
            "8  'A special place': Chi-Chi's restaurants, popu...   \n",
            "9  Social media platform Bluesky nearing 25 milli...   \n",
            "\n",
            "                                         Article Url  \\\n",
            "0  https://www.usatoday.com/story/news/factcheck/...   \n",
            "1  https://www.usatoday.com/story/news/factcheck/...   \n",
            "2  https://www.usatoday.com/story/news/factcheck/...   \n",
            "3  https://www.usatoday.com/story/money/retail/20...   \n",
            "4  https://www.usatoday.com/story/news/factcheck/...   \n",
            "5  https://www.usatoday.com/story/news/weather/20...   \n",
            "6  https://www.usatoday.com/story/news/world/2024...   \n",
            "7  https://www.usatoday.com/story/money/cars/reca...   \n",
            "8  https://www.usatoday.com/story/money/food/2024...   \n",
            "9  https://www.usatoday.com/story/tech/2024/12/06...   \n",
            "\n",
            "                                    Threads Post URL             Username  \\\n",
            "0  https://www.threads.net/@robertellingsworth/po...  @robertellingsworth   \n",
            "1  https://www.threads.net/@indyeric10/post/DDFzi...          @indyeric10   \n",
            "2  https://www.threads.net/@robertnagel_/post/DC-...        @robertnagel_   \n",
            "3                                                NaN                 None   \n",
            "4  https://www.threads.net/@greenemachine1970/pos...   @greenemachine1970   \n",
            "5                                                NaN                 None   \n",
            "6                                                NaN                 None   \n",
            "7                                                NaN                 None   \n",
            "8                                                NaN                 None   \n",
            "9                                                NaN                 None   \n",
            "\n",
            "          Code  \n",
            "0  DDK-2l1vcN2  \n",
            "1  DDFzimKS4PF  \n",
            "2  DC-hNsoRaR8  \n",
            "3         None  \n",
            "4  DC8NNuSuY_Q  \n",
            "5         None  \n",
            "6         None  \n",
            "7         None  \n",
            "8         None  \n",
            "9         None  \n",
            "Updated dataset saved to usaToday_updated.csv\n"
          ]
        }
      ],
      "id": "C454xskxJq8e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Labelling**"
      ],
      "metadata": {
        "id": "r3wrQgDaB-cU"
      },
      "id": "r3wrQgDaB-cU"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read dataset1 and dataset2 from CSV files with a specified encoding\n",
        "dataset1 = pd.read_csv('/content/ThreadsDataset.csv', encoding='ISO-8859-1')  # Try ISO-8859-1\n",
        "dataset2 = pd.read_csv('/content/FakeData Combined.csv', encoding='ISO-8859-1')  # Try ISO-8859-1\n",
        "\n",
        "# Add a label column to dataset1\n",
        "dataset1['label'] = dataset1['code'].isin(dataset2['Code']).astype(int)\n",
        "\n",
        "print(dataset1.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlg964D9VKse",
        "outputId": "d3822340-c309-49e6-a981-db3912f635ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        caption/text         code  \\\n",
            "0  https://eriecountypa.gov/wp-content/uploads/20...  DCZLqITOaIa   \n",
            "1  Cool! Pennsylvania decided to count the legal ...  DCai0PruJwV   \n",
            "2                                                NaN  DCaf0tFO_Bc   \n",
            "3  795 votes were not counted in Pennsylvania bec...  DCZim40v9Aa   \n",
            "4  Never been more ashamed to be an American! Dou...  DCVLvJGOqDL   \n",
            "5  So we weren?t allowed to cause a scene and fil...  DCVKUEMu__l   \n",
            "6                                                NaN  DCUPV2nvDi8   \n",
            "7         Project 2025. ( that Trump never heard of)  DCUFG6ouOwQ   \n",
            "8                         @steelergal126.bsky.social  DCSxl8muiMG   \n",
            "9  Oh my lord! Was strolling Twitter.. can?t help...  DCSngtau9lp   \n",
            "\n",
            "                                id  like_count            pk  \\\n",
            "0  3501881458899395098_69378095191         4.0  3.500000e+18   \n",
            "1  3502264784067795989_69378095191       373.0  3.500000e+18   \n",
            "2  3502251621494812764_69378095191         0.0  3.500000e+18   \n",
            "3  3501982391193292826_69378095191        12.0  3.500000e+18   \n",
            "4  3500755903445639371_69378095191        13.0  3.500000e+18   \n",
            "5  3500749644713689061_69378095191        19.0  3.500000e+18   \n",
            "6  3500490282829101244_69378095191         1.0  3.500000e+18   \n",
            "7  3500445275883367440_69378095191        14.0  3.500000e+18   \n",
            "8  3500077972394222342_69378095191         2.0  3.500000e+18   \n",
            "9  3500033632024385897_69378095191        23.0  3.500000e+18   \n",
            "\n",
            "                                profileUrl  reply_count      taken_at  \\\n",
            "0  https://www.threads.net/@steelergal1215          3.0  1.731677e+09   \n",
            "1  https://www.threads.net/@steelergal1215          6.0  1.731723e+09   \n",
            "2  https://www.threads.net/@steelergal1215         12.0  1.731721e+09   \n",
            "3  https://www.threads.net/@steelergal1215          1.0  1.731689e+09   \n",
            "4  https://www.threads.net/@steelergal1215          NaN  1.731543e+09   \n",
            "5  https://www.threads.net/@steelergal1215          1.0  1.731542e+09   \n",
            "6  https://www.threads.net/@steelergal1215          3.0  1.731511e+09   \n",
            "7  https://www.threads.net/@steelergal1215          NaN  1.731506e+09   \n",
            "8  https://www.threads.net/@steelergal1215          1.0  1.731462e+09   \n",
            "9  https://www.threads.net/@steelergal1215          1.0  1.731457e+09   \n",
            "\n",
            "  text_post_app_info/share_info/reposted_post/caption/text       user/id  \\\n",
            "0                                                NaN        6.937810e+10   \n",
            "1                                                NaN        6.937810e+10   \n",
            "2  I know a truck driver that transported a trail...        6.937810e+10   \n",
            "3                                                NaN        6.937810e+10   \n",
            "4                                                NaN        6.937810e+10   \n",
            "5                                                NaN        6.937810e+10   \n",
            "6  WATCH....All you ever need to do in response t...        6.937810e+10   \n",
            "7                                                NaN        6.937810e+10   \n",
            "8                                                NaN        6.937810e+10   \n",
            "9                                                NaN        6.937810e+10   \n",
            "\n",
            "  user/is_verified       user/pk  \\\n",
            "0            False  6.937810e+10   \n",
            "1            False  6.937810e+10   \n",
            "2            False  6.937810e+10   \n",
            "3            False  6.937810e+10   \n",
            "4            False  6.937810e+10   \n",
            "5            False  6.937810e+10   \n",
            "6            False  6.937810e+10   \n",
            "7            False  6.937810e+10   \n",
            "8            False  6.937810e+10   \n",
            "9            False  6.937810e+10   \n",
            "\n",
            "                                user/profile_pic_url   user/username  \\\n",
            "0  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "1  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "2  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "3  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "4  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "5  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "6  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "7  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "8  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "9  https://scontent-ber1-1.cdninstagram.com/v/t51...  steelergal1215   \n",
            "\n",
            "   Unnamed: 14  Unnamed: 15  label  \n",
            "0          NaN          NaN      0  \n",
            "1          NaN          NaN      0  \n",
            "2          NaN          NaN      0  \n",
            "3          NaN          NaN      0  \n",
            "4          NaN          NaN      0  \n",
            "5          NaN          NaN      0  \n",
            "6          NaN          NaN      0  \n",
            "7          NaN          NaN      0  \n",
            "8          NaN          NaN      0  \n",
            "9          NaN          NaN      0  \n"
          ]
        }
      ],
      "id": "Rlg964D9VKse"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated dataset1 to a new CSV file\n",
        "dataset1.to_csv('labeled_data.csv', index=False)\n",
        "print(\"Labeled dataset1 saved as 'labeled_data.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjG7tT55WCk-",
        "outputId": "2588c4f3-5b0c-4aae-f1b4-f25cfdf4d0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeled dataset1 saved as 'labeled_data.csv'.\n"
          ]
        }
      ],
      "id": "wjG7tT55WCk-"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}