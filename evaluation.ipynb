{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3204508438.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install openai==0.28\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3280527727.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install ipywidgets\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import nest_asyncio\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\llama-4_vs_deepseek-r1\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index opik nest_asyncio python-dotenv pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at C:\\Users\\acer\\.opik.config\n"
     ]
    }
   ],
   "source": [
    "import opik\n",
    "opik.configure(use_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\llama-4_vs_deepseek-r1\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load your OpenAI API key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\llama-4_vs_deepseek-r1\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "\n",
    "def load_llm(model_option):\n",
    "    if model_option == \"Llama-4\":\n",
    "        llm = Groq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    else:\n",
    "        llm = Groq(model=\"deepseek-r1-distill-llama-70b\", api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Llama-4'\n",
    "# model_name = 'DeepSeek-R1'\n",
    "llm  = load_llm(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace RAG calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from opik.integrations.llama_index import LlamaIndexCallbackHandler\n",
    "\n",
    "# A callback handler tp automatically log all LlamaIndex operations to Opik\n",
    "opik_callback_handler = LlamaIndexCallbackHandler()\n",
    "\n",
    "# Integrate handler into LlamaIndex's settings\n",
    "Settings.callback_manager = CallbackManager([opik_callback_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from opik import Opik\n\nclient = Opik()\ndataset = client.get_or_create_dataset(name=\"Fake News Detection Dataset\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\ndf = pd.read_csv(\"crowd_sourced_balanced_dataset.csv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for fake news detection evaluation\n# Adjust column names based on your dataset structure\n\ntext_col = df.columns[0]  # Adjust if needed\nlabel_col = df.columns[1]  # Adjust if needed\n\nfake_news_pairs = [\n    {\n        \"input\": row[text_col], \n        \"expected_output\": \"FAKE\" if row[label_col] == 1 else \"REAL\",\n        \"context\": row[text_col]\n    } \n    for _, row in df.head(100).iterrows()  # Using first 100 for evaluation\n]\nfake_news_pairs[0]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use insert if you're creating the dataset for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "dataset.insert(fake_news_pairs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\n\n# Load the trained fake news detection model\nwith open('fake_news_model.pkl', 'rb') as f:\n    fake_news_model = pickle.load(f)\nwith open('tfidf_vectorizer.pkl', 'rb') as f:\n    tfidf_vectorizer = pickle.load(f)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from opik import track\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef preprocess_text(text):\n    \"\"\"Preprocess text for fake news detection\"\"\"\n    text = text.lower()\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    words = [word for word in words if word not in stop_words]\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n    return ' '.join(words)\n\n@track\ndef fake_news_classifier(input: str) -> str:\n    processed_text = preprocess_text(input)\n    text_vectorized = tfidf_vectorizer.transform([processed_text])\n    prediction = fake_news_model.predict(text_vectorized)[0]\n    return \"FAKE\" if prediction == 1 else \"REAL\"\n\ndef evaluation_task(x):\n    return {\n        \"output\": fake_news_classifier(x['input'])\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from opik.evaluation.metrics import (\n    Equals,\n    AnswerRelevance\n)\n\n# Define the metrics for fake news detection\nequals_metric = Equals()  # Exact match for classification\nanswer_relevance_metric = AnswerRelevance()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from opik.evaluation import evaluate\n\nevaluation = evaluate(\n    dataset=dataset,\n    task=evaluation_task,\n    experiment_name=\"Fake News Detection - \" + model_name,\n    scoring_metrics=[equals_metric, answer_relevance_metric],\n    experiment_config={\n        \"model\": \"Logistic Regression + TF-IDF\"\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}